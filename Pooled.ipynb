{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3786b3ed-30b7-4982-9ab3-f5686d0a1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.132.0)\n",
      "Requirement already satisfied: google-auth in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.45.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (2.3.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: docstring_parser<1 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (0.17.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (6.33.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (3.39.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: shapely<3.0.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (2.1.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (1.27.0)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (25.0)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (1.56.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (4.15.0)\n",
      "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (3.7.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform) (2.12.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth) (0.4.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth) (6.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth) (4.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.4)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.8.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform) (1.8.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.9.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.7.14)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ankit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ankit\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-aiplatform google-auth pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2312777-76bc-47e0-ac09-cd56743160c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#population extracton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd85f38-bc0f-4635-9c7f-f658ee42837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 320682.jpg -> excel_output\\320682.xlsx\n",
      "ðŸŽ¯ Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# IMPORTS\n",
    "# =========================\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.oauth2 import service_account\n",
    "import vertexai\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, ConfigDict\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AUTH CONFIG\n",
    "# =========================\n",
    "SERVICE_ACCOUNT_FILE = \"vigilant-armor-455313-m8-54c19548a094.json\"\n",
    "PROJECT_ID = \"vigilant-armor-455313-m8\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE\n",
    ").with_scopes([\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PYDANTIC SCHEMA (UPDATED FOR NEW MULTI-TRIAL + STRING-ONLY OUTPUT)\n",
    "# =========================\n",
    "ArmType = Literal[\n",
    "    \"Experimental\",\n",
    "    \"Comparator\",\n",
    "    \"Single-arm\",\n",
    "    \"External control\",\n",
    "    \"Dose level\",\n",
    "    \"Cohort\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "PopulationType = Literal[\n",
    "    \"Overall\",\n",
    "    \"Analysis set\",\n",
    "    \"Cohort\",\n",
    "    \"Baseline characteristic\",\n",
    "    \"Subgroup\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "IntegratedType = Literal[\n",
    "    \"Integrated population\",\n",
    "    \"Pooled analysis\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "\n",
    "class DesignSummary(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    type: Optional[str] = None\n",
    "\n",
    "\n",
    "class TrialPopulationDetails(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    type: Optional[str] = None\n",
    "\n",
    "\n",
    "class TrialRecord(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    trial_key: str  # e.g., \"TRIAL_1\"\n",
    "    trial_id_list: List[str] = Field(default_factory=list)\n",
    "    trial_label: Optional[str] = None\n",
    "    phase: Optional[str] = None\n",
    "    study_name: Optional[str] = None\n",
    "    allocation: Optional[str] = None\n",
    "    design_summary: DesignSummary\n",
    "    trial_population_details: TrialPopulationDetails\n",
    "    overall_N: Optional[str] = None  # STRING (e.g., \"312\") or null\n",
    "\n",
    "\n",
    "class ArmRecord(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    arm_key: str  # e.g., \"ARM_1\"\n",
    "    arm_name: Optional[str] = None\n",
    "    arm_type: Optional[ArmType] = None\n",
    "    treatment_description: Optional[str] = None\n",
    "    dose_schedule: Optional[str] = None\n",
    "\n",
    "\n",
    "class PopulationRecord(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    population_key: str  # e.g., \"POP_1\"\n",
    "    population_type: PopulationType\n",
    "    parent: Optional[str] = None\n",
    "    child: Optional[str] = None\n",
    "    population_description: Optional[str] = None\n",
    "    N: Optional[str] = None  # STRING (e.g., \"45\") or null\n",
    "\n",
    "\n",
    "class TrialArmLink(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    trial_key: str\n",
    "    linked_arm_keys: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class TrialPopulationLink(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    trial_key: str\n",
    "    linked_population_keys: List[str] = Field(default_factory=list)\n",
    "    linked_arm_keys: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class IntegratedRecord(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    integrated_key: str  # e.g., \"INTEGRATED_1\"\n",
    "    integrated_type: IntegratedType\n",
    "    source_trial_keys: List[str] = Field(default_factory=list)\n",
    "    population_description: Optional[str] = None\n",
    "    N: Optional[str] = None  # STRING or null\n",
    "    linked_population_keys: List[str] = Field(default_factory=list)\n",
    "    linked_arm_keys: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class MultiTrialExtractionOutput(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    trial_records: List[TrialRecord] = Field(default_factory=list)\n",
    "    arm_records: List[ArmRecord] = Field(default_factory=list)\n",
    "    population_records: List[PopulationRecord] = Field(default_factory=list)\n",
    "    trial_arm_links: List[TrialArmLink] = Field(default_factory=list)\n",
    "    trial_population_links: List[TrialPopulationLink] = Field(default_factory=list)\n",
    "    integrated_records: List[IntegratedRecord] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD PROMPT FROM TXT\n",
    "# =========================\n",
    "PROMPT_FILE = \"Pooled_Population.txt\"  # put your updated markdown prompt here\n",
    "with open(PROMPT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_text = f.read()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "input_folder = \"images_input\"\n",
    "json_output = \"json_output\"\n",
    "excel_output = \"excel_output\"\n",
    "\n",
    "os.makedirs(json_output, exist_ok=True)\n",
    "os.makedirs(excel_output, exist_ok=True)\n",
    "\n",
    "image_files = [\n",
    "    f for f in os.listdir(input_folder)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\"))\n",
    "]\n",
    "\n",
    "\n",
    "def _safe_json_text(model_text: str, image_file: str) -> str:\n",
    "    txt = (model_text or \"\").strip()\n",
    "    if not (txt.startswith(\"{\") and txt.endswith(\"}\")):\n",
    "        raise RuntimeError(f\"Model output not valid JSON / truncated for {image_file}:\\n{txt[:500]}\")\n",
    "    return txt\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PROCESS IMAGES\n",
    "# =========================\n",
    "for image_file in image_files:\n",
    "    image_id = os.path.splitext(image_file)[0]\n",
    "    image_path = os.path.join(input_folder, image_file)\n",
    "\n",
    "    # Decide MIME based on extension\n",
    "    ext = os.path.splitext(image_file)[1].lower()\n",
    "    mime = \"image/jpeg\"\n",
    "    if ext == \".png\":\n",
    "        mime = \"image/png\"\n",
    "    elif ext == \".webp\":\n",
    "        mime = \"image/webp\"\n",
    "\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "\n",
    "    image_part = types.Part.from_bytes(data=image_bytes, mime_type=mime)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=[prompt_text, image_part],\n",
    "        config={\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_json_schema\": MultiTrialExtractionOutput.model_json_schema(),\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "    )\n",
    "\n",
    "    raw_json = _safe_json_text(response.text, image_file)\n",
    "\n",
    "    try:\n",
    "        parsed = MultiTrialExtractionOutput.model_validate_json(raw_json)\n",
    "    except ValidationError as e:\n",
    "        raise RuntimeError(f\"Schema validation failed for {image_file}\\n{e}\")\n",
    "\n",
    "    # =========================\n",
    "    # SAVE JSON\n",
    "    # =========================\n",
    "    with open(os.path.join(json_output, f\"{image_id}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_json)\n",
    "\n",
    "    # =========================\n",
    "    # SAVE EXCEL (FLATTEN NESTED OBJECTS)\n",
    "    # =========================\n",
    "    out_xlsx = os.path.join(excel_output, f\"{image_id}.xlsx\")\n",
    "    with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as writer:\n",
    "\n",
    "        # trial_records (flatten design_summary.* and trial_population_details.*)\n",
    "        trial_df = pd.json_normalize([t.model_dump() for t in parsed.trial_records])\n",
    "        trial_df.to_excel(writer, sheet_name=\"trial_records\", index=False)\n",
    "\n",
    "        # arm_records\n",
    "        arm_df = pd.DataFrame([a.model_dump() for a in parsed.arm_records])\n",
    "        arm_df.to_excel(writer, sheet_name=\"arm_records\", index=False)\n",
    "\n",
    "        # population_records\n",
    "        pop_df = pd.DataFrame([p.model_dump() for p in parsed.population_records])\n",
    "        pop_df.to_excel(writer, sheet_name=\"population_records\", index=False)\n",
    "\n",
    "        # trial_arm_links\n",
    "        tal_df = pd.DataFrame([x.model_dump() for x in parsed.trial_arm_links])\n",
    "        tal_df.to_excel(writer, sheet_name=\"trial_arm_links\", index=False)\n",
    "\n",
    "        # trial_population_links\n",
    "        tpl_df = pd.DataFrame([x.model_dump() for x in parsed.trial_population_links])\n",
    "        tpl_df.to_excel(writer, sheet_name=\"trial_population_links\", index=False)\n",
    "\n",
    "        # integrated_records\n",
    "        integ_df = pd.DataFrame([x.model_dump() for x in parsed.integrated_records])\n",
    "        integ_df.to_excel(writer, sheet_name=\"integrated_records\", index=False)\n",
    "\n",
    "    print(f\"âœ… Processed {image_file} -> {out_xlsx}\")\n",
    "\n",
    "print(\"ðŸŽ¯ Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdee7f-8cd6-4ac0-b541-10789e1942df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#km extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d49cd3-a964-4ab3-ae1a-6318939cd2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Survival extraction complete and validated.\n",
      "Saved JSON: survival_output.json\n",
      "âœ… JSON successfully converted to Excel: survival_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# UPDATED FOR NEW SCHEMA CHANGE:\n",
    "# - Added \"trial_label\" to each arm_level_survival_outcomes row\n",
    "# - Excel export includes trial_label (resolved from trial_metadata if missing)\n",
    "#\n",
    "# OUTPUT STRICTLY LIMITED TO:\n",
    "# {\n",
    "#   \"trial_metadata\": {...},\n",
    "#   \"arm_level_survival_outcomes\": [...]\n",
    "# }\n",
    "###############################################################\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Literal\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 0) PLACEHOLDERS (EDIT THESE)\n",
    "# =============================================================\n",
    "SERVICE_ACCOUNT_FILE = \"vigilant-armor-455313-m8-54c19548a094.json\"\n",
    "PROJECT_ID = \"vigilant-armor-455313-m8\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "\n",
    "PROMPT_TXT_FILE = \"KM.txt\"\n",
    "POSTER_IMAGE_PATH = \"images_input/320682.jpg\"\n",
    "INPUT2_SCHEMA_JSON_PATH = \"json_output/320682.json\"\n",
    "\n",
    "OUTPUT_JSON_PATH = \"survival_output.json\"\n",
    "OUTPUT_EXCEL_PATH = \"survival_output.xlsx\"\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 1) PYDANTIC OUTPUT SCHEMA (ONLY THE 2 KEYS YOU WANT)\n",
    "#    NEW: trial_label added to outcomes row\n",
    "# =============================================================\n",
    "PopulationType = Literal[\"Overall\", \"Analysis set\", \"Cohort\", \"Subgroup\", \"Other\"]\n",
    "TimeUnit = Literal[\"months\", \"years\", \"weeks\", \"days\"]\n",
    "\n",
    "class TrialMetadata(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    trial_id: Optional[str] = None\n",
    "    phase: Optional[str] = None\n",
    "    study_name: Optional[str] = None\n",
    "    # NOTE: trial_label is NOT in trial_metadata in your current output schema.\n",
    "    # We'll keep it out to match your \"trial_metadata\" schema exactly.\n",
    "\n",
    "\n",
    "class ArmLevelSurvivalOutcome(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    survival_outcome_id: int\n",
    "    trial_id: Optional[str] = None\n",
    "    trial_label: Optional[str] = None   # âœ… NEW\n",
    "    arm_description: Optional[str] = None\n",
    "\n",
    "    population_type: PopulationType\n",
    "    population_description: Optional[str] = None\n",
    "\n",
    "    endpoint_description: Optional[str] = None\n",
    "    endpoint_name: Optional[str] = None\n",
    "    endpoint_label: Optional[str] = None\n",
    "\n",
    "    assessment_type: Optional[str] = None\n",
    "    review_board: Optional[str] = None\n",
    "    review_criteria: Optional[str] = None\n",
    "    other_details: Optional[str] = None\n",
    "\n",
    "    arm_n: Optional[int] = None\n",
    "    median_survival: Optional[str] = None\n",
    "    survival_rate: Optional[str] = None\n",
    "    events_n: Optional[int] = None\n",
    "    assessment_denominator_n: Optional[int] = None\n",
    "\n",
    "    p_value: Optional[float] = None\n",
    "    time_unit: Optional[TimeUnit] = None\n",
    "\n",
    "    # ---------------- Validators ----------------\n",
    "    @field_validator(\"survival_outcome_id\")\n",
    "    @classmethod\n",
    "    def survival_id_positive(cls, v: int) -> int:\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"survival_outcome_id must be >= 1\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"arm_n\", \"events_n\", \"assessment_denominator_n\")\n",
    "    @classmethod\n",
    "    def non_negative_ints(cls, v: Optional[int]) -> Optional[int]:\n",
    "        if v is None:\n",
    "            return v\n",
    "        if v < 0:\n",
    "            raise ValueError(\"Count fields must be >= 0\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"p_value\")\n",
    "    @classmethod\n",
    "    def p_value_range(cls, v: Optional[float]) -> Optional[float]:\n",
    "        if v is None:\n",
    "            return v\n",
    "        if v < 0 or v > 1:\n",
    "            raise ValueError(\"p_value must be between 0 and 1\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class SurvivalOutput(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    trial_metadata: TrialMetadata = Field(default_factory=TrialMetadata)\n",
    "    arm_level_survival_outcomes: List[ArmLevelSurvivalOutcome] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 2) HELPERS\n",
    "# =============================================================\n",
    "def load_text(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "def load_json(path: str) -> Dict[str, Any]:\n",
    "    return json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def load_image_part(image_path: str) -> Part:\n",
    "    image_bytes = Path(image_path).read_bytes()\n",
    "    suffix = Path(image_path).suffix.lower()\n",
    "    if suffix in [\".jpg\", \".jpeg\"]:\n",
    "        mime = \"image/jpeg\"\n",
    "    elif suffix == \".png\":\n",
    "        mime = \"image/png\"\n",
    "    elif suffix == \".webp\":\n",
    "        mime = \"image/webp\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image type: {suffix}. Use jpg/png/webp.\")\n",
    "    return Part.from_data(data=image_bytes, mime_type=mime)\n",
    "\n",
    "def extract_first_json_object(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract the first JSON object from model text output.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        raise ValueError(\"Empty model output text.\")\n",
    "\n",
    "    # If pure JSON already\n",
    "    if text.startswith(\"{\") and text.endswith(\"}\"):\n",
    "        return json.loads(text)\n",
    "\n",
    "    # Try to find first {...}\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in model output.\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "def build_prompt_from_txt(prompt_txt: str, input2_schema: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Your KM.txt may already contain {INPUT_2_SCHEMA_JSON}.\n",
    "    If not, we append the INPUT 2 schema at the end.\n",
    "    \"\"\"\n",
    "    schema_str = json.dumps(input2_schema, ensure_ascii=False, indent=2)\n",
    "    if \"{INPUT_2_SCHEMA_JSON}\" in prompt_txt:\n",
    "        return prompt_txt.replace(\"{INPUT_2_SCHEMA_JSON}\", schema_str)\n",
    "    return prompt_txt + \"\\n\\nINPUT 2 JSON schema reference:\\n\" + schema_str\n",
    "\n",
    "\n",
    "# ---------------- Strict filtering (TOP + ROW) ----------------\n",
    "ALLOWED_TM_KEYS = {\"trial_id\", \"phase\", \"study_name\"}\n",
    "\n",
    "ALLOWED_OUTCOME_KEYS = {\n",
    "    \"survival_outcome_id\",\n",
    "    \"trial_id\",\n",
    "    \"trial_label\",           # âœ… NEW\n",
    "    \"arm_description\",\n",
    "    \"population_type\",\n",
    "    \"population_description\",\n",
    "    \"endpoint_description\",\n",
    "    \"endpoint_name\",\n",
    "    \"endpoint_label\",\n",
    "    \"assessment_type\",\n",
    "    \"review_board\",\n",
    "    \"review_criteria\",\n",
    "    \"other_details\",\n",
    "    \"arm_n\",\n",
    "    \"median_survival\",\n",
    "    \"survival_rate\",\n",
    "    \"events_n\",\n",
    "    \"assessment_denominator_n\",\n",
    "    \"p_value\",\n",
    "    \"time_unit\",\n",
    "}\n",
    "\n",
    "def _ensure_list(x: Any) -> List[Any]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return [x]\n",
    "    return []\n",
    "\n",
    "def _prune_dict(d: Any, allowed: set) -> Dict[str, Any]:\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    return {k: d.get(k) for k in allowed if k in d}\n",
    "\n",
    "def clean_to_survival_only(parsed: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    STRICT: Output must be ONLY:\n",
    "      {\"trial_metadata\": {...}, \"arm_level_survival_outcomes\": [...]}\n",
    "    Also prune nested dicts to avoid extra keys causing extra=\"forbid\" errors.\n",
    "    \"\"\"\n",
    "    tm_raw = parsed.get(\"trial_metadata\", {}) or {}\n",
    "    rows_raw = parsed.get(\"arm_level_survival_outcomes\", [])\n",
    "\n",
    "    tm = _prune_dict(tm_raw, ALLOWED_TM_KEYS)\n",
    "\n",
    "    rows = _ensure_list(rows_raw)\n",
    "    cleaned_rows = []\n",
    "    for r in rows:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        cleaned_rows.append(_prune_dict(r, ALLOWED_OUTCOME_KEYS))\n",
    "\n",
    "    return {\"trial_metadata\": tm, \"arm_level_survival_outcomes\": cleaned_rows}\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 3) AUTH + MODEL INIT\n",
    "# =============================================================\n",
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)\n",
    "model = GenerativeModel(MODEL_NAME)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 4) LOAD INPUTS (2 inputs)\n",
    "# =============================================================\n",
    "prompt_txt = load_text(PROMPT_TXT_FILE)\n",
    "input2_schema = load_json(INPUT2_SCHEMA_JSON_PATH)\n",
    "final_prompt = build_prompt_from_txt(prompt_txt, input2_schema)\n",
    "\n",
    "image_part = load_image_part(POSTER_IMAGE_PATH)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 5) CALL MODEL\n",
    "# =============================================================\n",
    "response = model.generate_content(\n",
    "    contents=[final_prompt, image_part],\n",
    "    generation_config={\"temperature\": 0.0},\n",
    ")\n",
    "\n",
    "raw_text = response.text or \"\"\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 6) PARSE + FILTER + (OPTIONAL) FILL trial_label + VALIDATE + SAVE\n",
    "# =============================================================\n",
    "parsed = extract_first_json_object(raw_text)\n",
    "\n",
    "# âœ… keep ONLY the 2 keys you requested (and prune extra keys inside them)\n",
    "filtered = clean_to_survival_only(parsed)\n",
    "\n",
    "# OPTIONAL: if the model didn't populate trial_label per row,\n",
    "# you can set it to None (leave as-is) OR copy from metadata if you add it there.\n",
    "# Since your trial_metadata schema has no trial_label, we do NOT inject anything.\n",
    "# We only ensure key exists if present; otherwise remains absent/None via Pydantic.\n",
    "\n",
    "validated = SurvivalOutput.model_validate(filtered, by_name=True)\n",
    "\n",
    "Path(OUTPUT_JSON_PATH).write_text(\n",
    "    validated.model_dump_json(indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Survival extraction complete and validated.\")\n",
    "print(f\"Saved JSON: {OUTPUT_JSON_PATH}\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 7) EXPORT TO EXCEL (Flatten outcomes with trial_metadata columns)\n",
    "# =============================================================\n",
    "data = json.loads(Path(OUTPUT_JSON_PATH).read_text(encoding=\"utf-8\"))\n",
    "trial_metadata = data.get(\"trial_metadata\", {}) or {}\n",
    "arm_outcomes = data.get(\"arm_level_survival_outcomes\", []) or []\n",
    "\n",
    "rows = []\n",
    "for row in arm_outcomes:\n",
    "    if not isinstance(row, dict):\n",
    "        continue\n",
    "    merged = {**trial_metadata, **row}\n",
    "    rows.append(merged)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_excel(OUTPUT_EXCEL_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… JSON successfully converted to Excel: {OUTPUT_EXCEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b890b3-0db2-4e17-b8b6-65fca1df36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basline pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afacb945-8032-4a82-9f7a-680b9bd476aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vigilant-armor-455313-m8-1d642ef84a8c.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 320\u001b[39m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mbc_types\u001b[39m\u001b[33m\"\u001b[39m: cleaned_rows}\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# 3) AUTH + MODEL INIT\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m credentials = \u001b[43mservice_account\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCredentials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_service_account_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSERVICE_ACCOUNT_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m vertexai.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)\n\u001b[32m    322\u001b[39m model = GenerativeModel(MODEL_NAME)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Larvol_WorkSpace\\Projects\\PosterPipeline\\venv\\Lib\\site-packages\\google\\oauth2\\service_account.py:262\u001b[39m, in \u001b[36mCredentials.from_service_account_file\u001b[39m\u001b[34m(cls, filename, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_service_account_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, **kwargs):\n\u001b[32m    252\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates a Credentials instance from a service account json file.\u001b[39;00m\n\u001b[32m    253\u001b[39m \n\u001b[32m    254\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    260\u001b[39m \u001b[33;03m            credentials.\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     info, signer = \u001b[43m_service_account_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_filename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclient_email\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken_uri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_signer_and_info(signer, info, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Larvol_WorkSpace\\Projects\\PosterPipeline\\venv\\Lib\\site-packages\\google\\auth\\_service_account_info.py:78\u001b[39m, in \u001b[36mfrom_filename\u001b[39m\u001b[34m(filename, require, use_rsa_signer)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_filename\u001b[39m(filename, require=\u001b[38;5;28;01mNone\u001b[39;00m, use_rsa_signer=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reads a Google service account JSON file and returns its parsed info.\u001b[39;00m\n\u001b[32m     66\u001b[39m \n\u001b[32m     67\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m \u001b[33;03m            info and a signer instance.\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[32m     79\u001b[39m         data = json.load(json_file)\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m data, from_dict(data, require=require, use_rsa_signer=use_rsa_signer)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'vigilant-armor-455313-m8-1d642ef84a8c.json'"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# UPDATED FOR NEW BASELINE CHARACTERISTICS SCHEMA (bc_types ONLY)\n",
    "#\n",
    "# Output STRICTLY LIMITED TO:\n",
    "# {\n",
    "#   \"bc_types\": [...]\n",
    "# }\n",
    "#\n",
    "# - Adds arm_key + population_key in output rows (per your new schema)\n",
    "# - Supports generated keys (arm_gen_1, pop_gen_1) in LLM output\n",
    "# - Validates with Pydantic (extra=\"forbid\")\n",
    "# - Exports bc_types to Excel (one row per bc_types record)\n",
    "###############################################################\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Literal\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 0) PLACEHOLDERS (EDIT THESE)\n",
    "# =============================================================\n",
    "SERVICE_ACCOUNT_FILE = \"vigilant-armor-455313-m8-1d642ef84a8c.json\"\n",
    "PROJECT_ID = \"vigilant-armor-455313-m8\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "\n",
    "PROMPT_TXT_FILE = \"Baseline.txt\"  # <-- your modified baseline prompt text file\n",
    "POSTER_IMAGE_PATH = \"images_input/NCT04171700_453611_4GT.jpg\"\n",
    "INPUT2_SCHEMA_JSON_PATH = \"json_output/NCT04171700_453611_4GT.json\"\n",
    "\n",
    "OUTPUT_JSON_PATH = \"baseline_output.json\"\n",
    "OUTPUT_EXCEL_PATH = \"baseline_output.xlsx\"\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 1) PYDANTIC OUTPUT SCHEMA (bc_types ONLY)\n",
    "# =============================================================\n",
    "PopulationType = Literal[\"Overall\", \"Analysis set\", \"Cohort\", \"Subgroup\", \"Other\"]\n",
    "BaselineParent = Literal[\"Overall\", \"Cohort\", \"Subgroup\", \"Other\", None]\n",
    "\n",
    "\n",
    "class BaselineCharacteristic(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    baseline_id: int = Field(..., description=\"Sequential ID starting from 1\")\n",
    "\n",
    "    trial_id: Optional[str] = None\n",
    "    trial_label: Optional[str] = None\n",
    "\n",
    "    arm_key: Optional[str] = None\n",
    "    arm_description: Optional[str] = None\n",
    "\n",
    "    population_key: Optional[str] = None\n",
    "    population_type: PopulationType\n",
    "    population_description: Optional[str] = None\n",
    "\n",
    "    baseline_parent: BaselineParent = None\n",
    "    parent_description: Optional[str] = None\n",
    "\n",
    "    baseline_category_label: Optional[str] = None\n",
    "    group_label: Optional[str] = None\n",
    "    group_text: Optional[str] = None\n",
    "\n",
    "    measure: Optional[str] = None\n",
    "    measure_value: Optional[str] = None\n",
    "\n",
    "    population_n: Optional[int] = None\n",
    "    population_percentage: Optional[float] = None\n",
    "\n",
    "    # ---------------- Validators ----------------\n",
    "    @field_validator(\"baseline_id\")\n",
    "    @classmethod\n",
    "    def baseline_id_positive(cls, v: int) -> int:\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"baseline_id must be >= 1\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"population_n\")\n",
    "    @classmethod\n",
    "    def non_negative_n(cls, v: Optional[int]) -> Optional[int]:\n",
    "        if v is None:\n",
    "            return v\n",
    "        if v < 0:\n",
    "            raise ValueError(\"population_n must be >= 0\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"population_percentage\")\n",
    "    @classmethod\n",
    "    def percent_range(cls, v: Optional[float]) -> Optional[float]:\n",
    "        if v is None:\n",
    "            return v\n",
    "        if v < 0 or v > 100:\n",
    "            raise ValueError(\"population_percentage must be between 0 and 100\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class BaselineOutput(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    bc_types: List[BaselineCharacteristic] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 2) HELPERS\n",
    "# =============================================================\n",
    "def load_text(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def load_json(path: str) -> Dict[str, Any]:\n",
    "    return json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "def load_image_part(image_path: str) -> Part:\n",
    "    image_bytes = Path(image_path).read_bytes()\n",
    "    suffix = Path(image_path).suffix.lower()\n",
    "    if suffix in [\".jpg\", \".jpeg\"]:\n",
    "        mime = \"image/jpeg\"\n",
    "    elif suffix == \".png\":\n",
    "        mime = \"image/png\"\n",
    "    elif suffix == \".webp\":\n",
    "        mime = \"image/webp\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image type: {suffix}. Use jpg/png/webp.\")\n",
    "    return Part.from_data(data=image_bytes, mime_type=mime)\n",
    "\n",
    "\n",
    "def extract_first_json_object(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract the first JSON object from model text output.\n",
    "    Handles cases where model wraps JSON in markdown fences or extra text.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        raise ValueError(\"Empty model output text.\")\n",
    "\n",
    "    # Remove markdown code fences if present\n",
    "    text = re.sub(r\"^```(?:json)?\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "\n",
    "    # If pure JSON already\n",
    "    if text.startswith(\"{\") and text.endswith(\"}\"):\n",
    "        return json.loads(text)\n",
    "\n",
    "    # Try to find first {...}\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in model output.\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "\n",
    "def build_prompt_from_txt(prompt_txt: str, input2_schema: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Your BC.txt may already contain {INPUT_2_SCHEMA_JSON}.\n",
    "    If not, we append the INPUT 2 schema at the end.\n",
    "    \"\"\"\n",
    "    schema_str = json.dumps(input2_schema, ensure_ascii=False, indent=2)\n",
    "    if \"{INPUT_2_SCHEMA_JSON}\" in prompt_txt:\n",
    "        return prompt_txt.replace(\"{INPUT_2_SCHEMA_JSON}\", schema_str)\n",
    "    return prompt_txt + \"\\n\\nINPUT 2 JSON schema reference:\\n\" + schema_str\n",
    "\n",
    "\n",
    "# ---------------- Strict filtering ----------------\n",
    "ALLOWED_TOP_KEYS = {\"bc_types\"}\n",
    "\n",
    "ALLOWED_BC_KEYS = {\n",
    "    \"baseline_id\",\n",
    "    \"trial_id\",\n",
    "    \"trial_label\",\n",
    "    \"arm_key\",\n",
    "    \"arm_description\",\n",
    "    \"population_key\",\n",
    "    \"population_type\",\n",
    "    \"population_description\",\n",
    "    \"baseline_parent\",\n",
    "    \"parent_description\",\n",
    "    \"baseline_category_label\",\n",
    "    \"group_label\",\n",
    "    \"group_text\",\n",
    "    \"measure\",\n",
    "    \"measure_value\",\n",
    "    \"population_n\",\n",
    "    \"population_percentage\",\n",
    "}\n",
    "\n",
    "\n",
    "def _ensure_list(x: Any) -> List[Any]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return [x]\n",
    "    return []\n",
    "\n",
    "\n",
    "def _prune_dict(d: Any, allowed: set) -> Dict[str, Any]:\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    return {k: d.get(k) for k in allowed if k in d}\n",
    "\n",
    "\n",
    "def _to_int_or_none(x: Any) -> Optional[int]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, int):\n",
    "        return x\n",
    "    if isinstance(x, float):\n",
    "        return int(x)\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        # pick first integer found (handles \"12 (34%)\" -> 12)\n",
    "        m = re.search(r\"-?\\d+\", s)\n",
    "        if not m:\n",
    "            return None\n",
    "        try:\n",
    "            return int(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _to_float_percent_or_none(x: Any) -> Optional[float]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        s = s.replace(\"%\", \"\").strip()\n",
    "        # allow decimal\n",
    "        m = re.search(r\"-?\\d+(?:\\.\\d+)?\", s)\n",
    "        if not m:\n",
    "            return None\n",
    "        try:\n",
    "            return float(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _normalize_population_type(v: Any) -> str:\n",
    "    \"\"\"\n",
    "    Keep strict allowed values. If missing/invalid, set to 'Other'\n",
    "    so Pydantic validation passes and output stays consistent.\n",
    "    \"\"\"\n",
    "    allowed = {\"Overall\", \"Analysis set\", \"Cohort\", \"Subgroup\", \"Other\"}\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip()\n",
    "        if s in allowed:\n",
    "            return s\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def _normalize_baseline_parent(v: Any) -> Optional[str]:\n",
    "    allowed = {\"Overall\", \"Cohort\", \"Subgroup\", \"Other\"}\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip()\n",
    "        if s in allowed:\n",
    "            return s\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_to_bc_only(parsed: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    STRICT: Output must be ONLY:\n",
    "      {\"bc_types\": [...]}\n",
    "\n",
    "    - Drops any extra top-level keys\n",
    "    - Prunes bc_types row keys\n",
    "    - Normalizes population_type/baseline_parent\n",
    "    - Parses numeric fields if model returns strings\n",
    "    \"\"\"\n",
    "    bc_raw = parsed.get(\"bc_types\", [])\n",
    "    rows = _ensure_list(bc_raw)\n",
    "\n",
    "    cleaned_rows: List[Dict[str, Any]] = []\n",
    "    for r in rows:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "\n",
    "        rr = _prune_dict(r, ALLOWED_BC_KEYS)\n",
    "\n",
    "        # Normalize required enums\n",
    "        rr[\"population_type\"] = _normalize_population_type(rr.get(\"population_type\"))\n",
    "        rr[\"baseline_parent\"] = _normalize_baseline_parent(rr.get(\"baseline_parent\"))\n",
    "\n",
    "        # Normalize numeric fields (allow model to output \"12\" or \"12%\" as strings)\n",
    "        rr[\"population_n\"] = _to_int_or_none(rr.get(\"population_n\"))\n",
    "        rr[\"population_percentage\"] = _to_float_percent_or_none(rr.get(\"population_percentage\"))\n",
    "\n",
    "        # baseline_id must exist; if missing, we will fill later in a second pass\n",
    "        cleaned_rows.append(rr)\n",
    "\n",
    "    # Fill baseline_id if missing / invalid\n",
    "    # (keeps deterministic ordering based on model output order)\n",
    "    for i, rr in enumerate(cleaned_rows, start=1):\n",
    "        bid = rr.get(\"baseline_id\")\n",
    "        if not isinstance(bid, int) or bid <= 0:\n",
    "            rr[\"baseline_id\"] = i\n",
    "\n",
    "    return {\"bc_types\": cleaned_rows}\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 3) AUTH + MODEL INIT\n",
    "# =============================================================\n",
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)\n",
    "model = GenerativeModel(MODEL_NAME)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 4) LOAD INPUTS (2 inputs)\n",
    "# =============================================================\n",
    "prompt_txt = load_text(PROMPT_TXT_FILE)\n",
    "input2_schema = load_json(INPUT2_SCHEMA_JSON_PATH)\n",
    "final_prompt = build_prompt_from_txt(prompt_txt, input2_schema)\n",
    "\n",
    "image_part = load_image_part(POSTER_IMAGE_PATH)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 5) CALL MODEL\n",
    "# =============================================================\n",
    "response = model.generate_content(\n",
    "    contents=[final_prompt, image_part],\n",
    "    generation_config={\"temperature\": 0.0},\n",
    ")\n",
    "\n",
    "raw_text = response.text or \"\"\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 6) PARSE + FILTER + VALIDATE + SAVE JSON\n",
    "# =============================================================\n",
    "parsed = extract_first_json_object(raw_text)\n",
    "filtered = clean_to_bc_only(parsed)\n",
    "\n",
    "validated = BaselineOutput.model_validate(filtered, by_name=True)\n",
    "\n",
    "Path(OUTPUT_JSON_PATH).write_text(\n",
    "    validated.model_dump_json(indent=2, exclude_none=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Baseline characteristics extraction complete and validated.\")\n",
    "print(f\"Saved JSON: {OUTPUT_JSON_PATH}\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 7) EXPORT TO EXCEL (bc_types only)\n",
    "# =============================================================\n",
    "data = json.loads(Path(OUTPUT_JSON_PATH).read_text(encoding=\"utf-8\"))\n",
    "bc_rows = data.get(\"bc_types\", []) or []\n",
    "\n",
    "df = pd.DataFrame(bc_rows)\n",
    "\n",
    "# Optional: keep a consistent column order\n",
    "preferred_cols = [\n",
    "    \"baseline_id\",\n",
    "    \"trial_id\",\n",
    "    \"trial_label\",\n",
    "    \"arm_key\",\n",
    "    \"arm_description\",\n",
    "    \"population_key\",\n",
    "    \"population_type\",\n",
    "    \"population_description\",\n",
    "    \"baseline_parent\",\n",
    "    \"parent_description\",\n",
    "    \"baseline_category_label\",\n",
    "    \"group_label\",\n",
    "    \"group_text\",\n",
    "    \"measure\",\n",
    "    \"measure_value\",\n",
    "    \"population_n\",\n",
    "    \"population_percentage\",\n",
    "]\n",
    "df = df.reindex(columns=[c for c in preferred_cols if c in df.columns])\n",
    "\n",
    "df.to_excel(OUTPUT_EXCEL_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… JSON successfully converted to Excel: {OUTPUT_EXCEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e437fd-5fe7-4d6f-81fb-4f7f7cff67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0f4ff6-1250-430b-b3f1-4866ae26283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.predict.instance_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.predict.instance_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.predict.params_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.predict.params_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.predict.prediction_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.predict.prediction_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.trainingjob.definition_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.trainingjob.definition_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.predict.instance_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.predict.instance_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.predict.params_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.predict.params_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.predict.prediction_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.predict.prediction_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.trainingjob.definition_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.trainingjob.definition_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response extraction complete and validated.\n",
      "Saved JSON: response_output.json\n",
      "âœ… JSON successfully converted to Excel with flattened result columns: response_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# UPDATED FOR RESPONSE OUTCOMES SCHEMA\n",
    "#\n",
    "# OUTPUT STRICTLY LIMITED TO:\n",
    "# {\n",
    "#   \"trial_metadata\": {...},\n",
    "#   \"arm_level_response_outcomes\": [...]\n",
    "# }\n",
    "###############################################################\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Literal\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 0) PLACEHOLDERS\n",
    "# =============================================================\n",
    "SERVICE_ACCOUNT_FILE = \"vigilant-armor-455313-m8-1d642ef84a8c.json\"\n",
    "PROJECT_ID = \"vigilant-armor-455313-m8\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "\n",
    "PROMPT_TXT_FILE = \"RESPONSE.txt\"\n",
    "POSTER_IMAGE_PATH = \"images_input/320682.jpg\"\n",
    "INPUT2_SCHEMA_JSON_PATH = \"json_output/320682.json\"\n",
    "\n",
    "OUTPUT_JSON_PATH = \"response_output.json\"\n",
    "OUTPUT_EXCEL_PATH = \"response_output.xlsx\"\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 1) PYDANTIC OUTPUT SCHEMA\n",
    "# =============================================================\n",
    "PopulationType = Literal[\"Overall\", \"Analysis set\", \"Cohort\", \"Subgroup\", \"Other\"]\n",
    "ResponseMetricClass = Literal[\"rate\", \"duration\", \"time_to_response\"]\n",
    "TimeUnit = Literal[\"months\", \"years\", \"weeks\", \"days\"]\n",
    "\n",
    "\n",
    "class TrialMetadata(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    trial_id: Optional[str] = None\n",
    "    phase: Optional[str] = None\n",
    "    study_name: Optional[str] = None\n",
    "\n",
    "\n",
    "class ResultObject(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    n: Optional[int] = None\n",
    "    percentage: Optional[float] = None\n",
    "    min: Optional[float] = None\n",
    "    max: Optional[float] = None\n",
    "\n",
    "    p_value: Optional[float] = None\n",
    "    odds_ratio: Optional[float] = None\n",
    "\n",
    "    median: Optional[float] = None\n",
    "    min_duration: Optional[float] = None\n",
    "    max_duration: Optional[float] = None\n",
    "    duration_unit: Optional[TimeUnit] = None\n",
    "\n",
    "\n",
    "class ArmLevelResponseOutcome(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    response_outcome_id: int\n",
    "\n",
    "    trial_id: Optional[str] = None\n",
    "    trial_label: Optional[str] = None\n",
    "\n",
    "    arm_description: Optional[str] = None\n",
    "\n",
    "    population_type: PopulationType\n",
    "    population_description: Optional[str] = None\n",
    "\n",
    "    assessment_type: Optional[str] = None\n",
    "    review_board: Optional[str] = None\n",
    "    review_criteria: Optional[str] = None\n",
    "    other_details: Optional[str] = None\n",
    "\n",
    "    arm_n: Optional[int] = None\n",
    "    assessment_denominator_n: Optional[int] = None\n",
    "\n",
    "    response_type_name: Optional[str] = None\n",
    "    response_metric_class: Optional[ResponseMetricClass] = None\n",
    "\n",
    "    result: Optional[ResultObject] = None\n",
    "\n",
    "    # ---------------- Validators ----------------\n",
    "    @field_validator(\"response_outcome_id\")\n",
    "    @classmethod\n",
    "    def id_positive(cls, v: int) -> int:\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"response_outcome_id must be >= 1\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"arm_n\", \"assessment_denominator_n\")\n",
    "    @classmethod\n",
    "    def non_negative_ints(cls, v: Optional[int]) -> Optional[int]:\n",
    "        if v is None:\n",
    "            return v\n",
    "        if v < 0:\n",
    "            raise ValueError(\"Count fields must be >= 0\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class ResponseOutput(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    trial_metadata: TrialMetadata = Field(default_factory=TrialMetadata)\n",
    "    arm_level_response_outcomes: List[ArmLevelResponseOutcome] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 2) HELPERS\n",
    "# =============================================================\n",
    "def load_text(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def load_json(path: str) -> Dict[str, Any]:\n",
    "    return json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "def load_image_part(image_path: str) -> Part:\n",
    "    image_bytes = Path(image_path).read_bytes()\n",
    "    suffix = Path(image_path).suffix.lower()\n",
    "    mime = \"image/jpeg\" if suffix in [\".jpg\", \".jpeg\"] else \"image/png\"\n",
    "    return Part.from_data(data=image_bytes, mime_type=mime)\n",
    "\n",
    "\n",
    "def extract_first_json_object(text: str) -> Dict[str, Any]:\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON found in model output\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "\n",
    "def build_prompt_from_txt(prompt_txt: str, input2_schema: Dict[str, Any]) -> str:\n",
    "    schema_str = json.dumps(input2_schema, ensure_ascii=False, indent=2)\n",
    "    return prompt_txt.replace(\"{INPUT_2_SCHEMA_JSON}\", schema_str)\n",
    "\n",
    "\n",
    "# ---------------- Strict filtering ----------------\n",
    "ALLOWED_TM_KEYS = {\"trial_id\", \"phase\", \"study_name\"}\n",
    "\n",
    "ALLOWED_OUTCOME_KEYS = {\n",
    "    \"response_outcome_id\",\n",
    "    \"trial_id\",\n",
    "    \"trial_label\",\n",
    "    \"arm_description\",\n",
    "    \"population_type\",\n",
    "    \"population_description\",\n",
    "    \"assessment_type\",\n",
    "    \"review_board\",\n",
    "    \"review_criteria\",\n",
    "    \"other_details\",\n",
    "    \"arm_n\",\n",
    "    \"assessment_denominator_n\",\n",
    "    \"response_type_name\",\n",
    "    \"response_metric_class\",\n",
    "    \"result\",\n",
    "}\n",
    "\n",
    "\n",
    "def _ensure_list(x: Any) -> List[Any]:\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return [x]\n",
    "    return []\n",
    "\n",
    "\n",
    "def _prune_dict(d: Any, allowed: set) -> Dict[str, Any]:\n",
    "    return {k: d.get(k) for k in allowed if isinstance(d, dict) and k in d}\n",
    "\n",
    "\n",
    "def clean_to_response_only(parsed: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    tm = _prune_dict(parsed.get(\"trial_metadata\", {}), ALLOWED_TM_KEYS)\n",
    "\n",
    "    rows = []\n",
    "    for r in _ensure_list(parsed.get(\"arm_level_response_outcomes\", [])):\n",
    "        rows.append(_prune_dict(r, ALLOWED_OUTCOME_KEYS))\n",
    "\n",
    "    return {\n",
    "        \"trial_metadata\": tm,\n",
    "        \"arm_level_response_outcomes\": rows,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 3) AUTH + MODEL INIT\n",
    "# =============================================================\n",
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)\n",
    "model = GenerativeModel(MODEL_NAME)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 4) LOAD INPUTS\n",
    "# =============================================================\n",
    "prompt_txt = load_text(PROMPT_TXT_FILE)\n",
    "input2_schema = load_json(INPUT2_SCHEMA_JSON_PATH)\n",
    "final_prompt = build_prompt_from_txt(prompt_txt, input2_schema)\n",
    "image_part = load_image_part(POSTER_IMAGE_PATH)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 5) CALL MODEL\n",
    "# =============================================================\n",
    "response = model.generate_content(\n",
    "    contents=[final_prompt, image_part],\n",
    "    generation_config={\"temperature\": 0.0},\n",
    ")\n",
    "\n",
    "raw_text = response.text or \"\"\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 6) PARSE + FILTER + VALIDATE + SAVE\n",
    "# =============================================================\n",
    "parsed = extract_first_json_object(raw_text)\n",
    "filtered = clean_to_response_only(parsed)\n",
    "\n",
    "validated = ResponseOutput.model_validate(filtered)\n",
    "\n",
    "Path(OUTPUT_JSON_PATH).write_text(\n",
    "    validated.model_dump_json(indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Response extraction complete and validated.\")\n",
    "print(f\"Saved JSON: {OUTPUT_JSON_PATH}\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 7) EXPORT TO EXCEL\n",
    "# =============================================================\n",
    "data = json.loads(Path(OUTPUT_JSON_PATH).read_text())\n",
    "tm = data.get(\"trial_metadata\", {})\n",
    "rows = data.get(\"arm_level_response_outcomes\", [])\n",
    "\n",
    "# =============================================================\n",
    "# 7) EXPORT TO EXCEL (FLATTEN result OBJECT)\n",
    "# =============================================================\n",
    "data = json.loads(Path(OUTPUT_JSON_PATH).read_text(encoding=\"utf-8\"))\n",
    "trial_metadata = data.get(\"trial_metadata\", {}) or {}\n",
    "arm_outcomes = data.get(\"arm_level_response_outcomes\", []) or []\n",
    "\n",
    "flattened_rows = []\n",
    "\n",
    "for row in arm_outcomes:\n",
    "    if not isinstance(row, dict):\n",
    "        continue\n",
    "\n",
    "    result = row.pop(\"result\", {}) or {}\n",
    "\n",
    "    flattened = {\n",
    "        **trial_metadata,\n",
    "        **row,\n",
    "\n",
    "        # ---- result fields flattened ----\n",
    "        \"result_n\": result.get(\"n\"),\n",
    "        \"result_percentage\": result.get(\"percentage\"),\n",
    "        \"result_min\": result.get(\"min\"),\n",
    "        \"result_max\": result.get(\"max\"),\n",
    "        \"result_p_value\": result.get(\"p_value\"),\n",
    "        \"result_odds_ratio\": result.get(\"odds_ratio\"),\n",
    "        \"result_median\": result.get(\"median\"),\n",
    "        \"result_min_duration\": result.get(\"min_duration\"),\n",
    "        \"result_max_duration\": result.get(\"max_duration\"),\n",
    "        \"result_duration_unit\": result.get(\"duration_unit\"),\n",
    "    }\n",
    "\n",
    "    flattened_rows.append(flattened)\n",
    "\n",
    "df = pd.DataFrame(flattened_rows)\n",
    "\n",
    "df.to_excel(OUTPUT_EXCEL_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… JSON successfully converted to Excel with flattened result columns: {OUTPUT_EXCEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
